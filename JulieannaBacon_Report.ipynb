{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b6da32e",
   "metadata": {},
   "source": [
    "# Final Project Report:\n",
    "## Galactic Distance Estimates from Photographs\n",
    "#### Author: Julieanna Bacon\n",
    "\n",
    "[Github Link!](https://github.com/JuLieAlgebra/cv_final_project)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91a29194",
   "metadata": {},
   "source": [
    "# Table of contents\n",
    "1. [Definitions](#def)\n",
    "1. [Problem Background](#introduction)\n",
    "2. [Project Overview](#paragraph1)\n",
    "    1. [Acquiring the Data](#gettingdata)\n",
    "        1. [Code](#download_code)\n",
    "    2. [Preprocessing](#preprocessing)\n",
    "        2. [Code](#preprocessing_code)\n",
    "3. [EDA](#eda)\n",
    "    1. [Preprocessed Data: Visualized](#eda_vis)\n",
    "    2. [Stats: Visualized](#eda_stats)\n",
    "4. [Architecture](#architecture)\n",
    "    1. [Code](#arch_code)\n",
    "    2. [Training Ex](#train_code)\n",
    "5. [Improving the Model](#improve)\n",
    "6. [Results](#results)\n",
    "7. [Sources](#sources)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ceddc91",
   "metadata": {},
   "source": [
    "### Definitions <a name=\"def\"></a>\n",
    "- Redshift – “distance” as measured by Doppler shift. \n",
    "- Hubble Constant – a time varying parameter that tells us the rate of expansion of the universe at that time epoch \n",
    "- CCD – the camera of choice for most telescopes \n",
    "- Spectroscopic Redshift – more accurate than photometric redshifts. Distance measurements produced by fitting spectra data to a black body curve \n",
    "- Photometric Redshift - distance measurements produced by photometric data \n",
    "- Photometric – Data produced by photos of objects. Usually taken in a filter to only capture photos in a given range of wavelengths (red, blue, green, etc). \n",
    "- SDSS – Sloan Digital Sky Survey"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c7b2d65",
   "metadata": {},
   "source": [
    "## Problem Background <a name=\"introduction\"></a>\n",
    "I loosely replicated (and modified) the smaller architecture from this paper: Photometric redshifts from SDSS images using a Convolutional Neural Network [2]. \n",
    "\n",
    "The overall goal of the project is to produce distance estimations of galaxies from photos in a learned approach. Below is a quick overview of the science behind why this is possible.\n",
    "\n",
    "How far away is this galaxy?\n",
    "\n",
    "<img src='docs/images/galaxy_dist.png' width=\"400\" height=\"400\">\n",
    "\n",
    "In astronomy, until we develop light speed engines, we’re almost entirely limited to estimating this by only cameras. It’s a surprisingly hard task that’s far from solved. It also underpins every aspect of research. From determining the age of the universe and resolving crises in fundamental physics to being able to calibrate solar convection models, it impacts everything.\n",
    "\n",
    "Astronomers have developed a variety of methods, used for different distances and different situations, but we will talk about the most accurate and its learned approximation.\n",
    "\n",
    "<img src='docs/images/slide1.png' width=\"700\" height=\"500\">  \n",
    "\n",
    "This method is called spectroscopic redshift. Spectroscopic, meaning it looks at the spectrum, and redshift, the distance to the galaxy. We’re all familiar with Doppler Shift, as we experience it every time an ambulance passes us on the street. It describes how waves, sound or light, get stretched out or compressed as objects move relative to each other. And galaxies are, unfortunately and fortunately, all moving away from us. This is because the universe itself is expanding, and like ants on a balloon being blown up, ants/galaxies farther away from us appear to moving faster away than nearby ants.\n",
    "\n",
    "<img src='docs/images/cosmic_doppler.png' width=\"400\" height=\"400\">  \n",
    "<!-- <img src='docs/images/cosmological_redshift.png' width=\"400\" height=\"400\"> \n",
    " -->\n",
    "With expansion, the light from galaxies is being stretched out, and the farther away from us, the more their light has been stretched out. This stretching out of light makes the whole object seem slightly redder (thus the term redshift). We can measure how the light has become stretched out, and thus how far away it is, through spectroscopy very precisely, as spectra shows us how bright the object is at every wavelength. If we know that the object will be super bright at wavelength 4268nm (for example) because it’s mostly made of hydrogen and hydrogen glows very brightly at that wavelength, we can see how much that hydrogen line has shifted when we observe the object.\n",
    "<p float=\"left\">\n",
    "  <img src='docs/images/spectra_elements.png' width=\"400\" />\n",
    "  <img src='docs/images/ex_spectrum.png' width=\"400\" /> \n",
    "</p>\n",
    "\n",
    "Unfortunately, getting spectra is a hard process that’s not always possible.\n",
    "\n",
    "<img src='docs/images/slide2.png' width=\"700\" height=\"400\">  \n",
    "\n",
    "The alternative is to see how bright the image via photos, which you can think of as a rough histogram of a spectrum. Spectra show brightness at every wavelength, photographs show brightness over a range of wavelengths. Astronomers use filters, like green, ultraviolet, etc to take photos in narrower wavelength bands than our phone cameras to see how bright something is over that histogram bin. Since there’s still information about how the object has been “reddened” in that rough histogram, astronomers can still get a (rougher) estimate of that reddening. This process is called Photometric Redshift.\n",
    "\n",
    "While with images instead of spectra, you won't be able to match up the the distinct emission lines from elements to \"aha!\" precisely how much the object has been reddened, there are still real, physically meaningful relationships that these machine learning methods can learn from this photometric data. A few of them are also fleshed out distance methods of their own: https://en.wikipedia.org/wiki/Cosmic_distance_ladder if you'd like to read more.\n",
    "\n",
    "<img src='docs/images/problem_overview.png' width=\"700\" height=\"400\">   \n",
    "\n",
    "For photometric redshifts, most astronomy pipelines today transform a cleaned image of an object in a particular filter to a single number, magnitude/brightness, that can then be used in a machine learning approach. There are other things calculated from these images that are used in other situations, but for redshift estimation, the work is done with usually five or more magnitudes in different, non-overlapping filters.\n",
    "\n",
    "I think this quote from a recent photometric redshift with CNNs paper sums up the promise of using the *entire* image instead of just the singular numbers:  \n",
    "\n",
    "\"A challenge for photo-z [photometric redshift] estimation methods that take magnitudes and colours as inputs is that there is not enough information...to break various degeneracies in the colour–redshift relation. One way to break these degeneracies is to include information about morphology, orientation, surface brightness, or visual appearance in general... A galaxy may appear red not just because its stellar population is instrinsically red but because it is a dusty edge-on spiral galaxy. Moreover, the fact that farther objects appear to be smaller and fainter to an observer also give us an additional piece of information to help break degeneracies. Most traditional methods for quantifying galaxy morphology, like ellipticity and Sérsic index, cannot fully encode all of the visual information that an image of a galaxy provides and hence methods that use images of galaxies directly as inputs ... and rely on artificial neural networks are the current state-of-the-art.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a10dde58",
   "metadata": {},
   "source": [
    "## Project Overview <a name=\"paragraph1\"></a>\n",
    "\n",
    "The goal of this project is to produce distance estimates for galaxies via images of them in different filter bands. Below is an illustration of the filter bands used by the SDSS, which is where we obtained all of our data.\n",
    "<img src='docs/images/sdss_filters.png' width=\"400\" height=\"400\">\n",
    "\n",
    "Originally I wanted to loosely replicate the Pasquet paper for a small dataset and play with different architectures, including a mixed-input model. I underestimated how long it would take to acquire and process all the data and produce a working model, so I did not get to the mixed-input modifications for the paper.\n",
    "\n",
    "In Pasquet et al, their main architecture is for 500,000+ galaxies, with them mentioning a smaller architecture that did fairly well on 10,000 galaxies. Due to hardware and time constraints, I chose the smaller dataset size, which still totalled to 80GB of compressed raw data. \n",
    "\n",
    "However, they didn't actually specify their smaller architecture or details like loss functions, optimizers, or parameters in the paper itself. I found their code repo that was meant to replicate their state of the art architecture at the time, but it performly extremely poorly out of the box. I detail that in the Results & Improving the Model sections.\n",
    "\n",
    "On the technical note, for most of the downloading and preprocessing, I used a framework called [Luigi](https://luigi.readthedocs.io/en/stable/). It allows you to actually parallelize your work and organize a set of steps (downloading, cleaning, cropping, etc) for Luigi to handle kicking off and ensuring that everything runs smoothing. It alerts you when something fails as well, you can also use it to run on servers, etc, it's very much like a Python-centric Airflow that's more hands on (and open source). \n",
    "\n",
    "With the 50,000+ images that I needed to download and process, it was pretty essential to my sanity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc10302e",
   "metadata": {},
   "source": [
    "## Acquiring the Data <a name=\"gettingdata\"></a>\n",
    "\n",
    "The tabular data and the image files are all available through the Sloan Digital Sky Survey (SDSS). They have a number of ways of acquiring both forms of data, though for bulk image data downloads, the recommendation is to use wget.\n",
    "\n",
    "I needed to create a list of all the objects I wanted in this dataset and from the information in that list, grab all of the images in the five filter bands I was interested in for each object (which are galaxies in this case). This involved submitting an SQL query to the CasJobs site. I used a different data release than the paper I’m (loosely) replicating and had different criteria for redshift range, number of objects that I could train on, and need to randomize the order. However, the database schema stayed relatively similar between the original paper and the data release I used (DR17 instead of DR12), so most of the query remains very similar to the original publish query.\n",
    "<img src='docs/images/sql.png' width=\"700\" height=\"400\">\n",
    "\n",
    "Once I had the results of that, I downloaded the database as a CSV and pushed it to my own S3 bucket. With a secure endpoint for these calculations, to make sure everything was perfectly repeatable (I have set seeds for everything), I wrote a S3 target task for luigi and a downloader for the tabular data (the results of the query).\n",
    "<img src='docs/images/tabular_data.png' width=\"900\" height=\"400\">\n",
    "\n",
    "Above you can see a snippet of the tabular data. LibreOffice Calc isn’t doing a great job of parsing the file, but pandas did everything correctly. From the columns camcol, field, and rerun, you can determine which FITS image the observation is in and grab each observation from formatting like this: f”https://data.sdss.org/sas/dr17/eboss/photoObj/frames/{row.rerun}/{row.run}/{row.camcol}/frame-{band}-{str(row.run).zfill(6)}-{row.camcol}-{str(row.field).zfill(4)}.fits.bz2n”\n",
    "\n",
    "Calling wget on that url will get you the right file. I generated a temporary file name based on the hash of the formatted url and told wget to download the file to that temp name, then when it was done, atomically “move” the file to its final name. I wrapped everything in a try except so it would continue if it ran into errors, since each downloading task is reponsible for downloading thousands of files.\n",
    "\n",
    "I also check to make sure that if the file already exists, not to download it. If the downloading failed, and the temporary file got deleted, I also tell luigi to re-attempt downloading.\n",
    "\n",
    "Also, unlike the Pasquet paper, I was getting galaxy brightness magnitudes across the different bands (for working on the mixed-input model). This involved some research to see what was the recommended magnitude measurements for galaxies from the SDSS. I decided to go with the cModel fluxes, which are discussed here: https://www.sdss.org/dr17/algorithms/magnitudes/#cmodel."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e0a6be9",
   "metadata": {},
   "source": [
    "#### Downloading Code:  <a name=\"download_code\"></a>\n",
    "Since a lot of this is wrapped up in Luigi tasks to define what the \"This step in the DAG successfully ran\" output file is, I'll paraphrase my code to the important computations. These two classes handle opening the (already downloaded earlier) tabular data file (csv with magnitudes, object ids, etc), formatting the template url for where the relevant FITS images are stored in the five available filter bands, then downloading them.\n",
    "\n",
    "Note that ImageDownloader is designed to run in parallel with multiple instances of itself responsible for downloading a smaller range of files than the 50,000 to do in total."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded799e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class URLgenerator(luigi.Task):\n",
    "    \"\"\"Luigi task to generate the URLS from the tabular data file\"\"\"\n",
    "    ...\n",
    "    def run(self):\n",
    "        \"\"\"Generates the URLs\"\"\"\n",
    "        df = pd.read_csv(self.input().path)\n",
    "        urls = []\n",
    "        bands = [\"u\", \"g\", \"r\", \"i\", \"z\"]\n",
    "        for i, row in df.iterrows():\n",
    "            for band in bands:\n",
    "                urls.append(\n",
    "                    f\"https://data.sdss.org/sas/dr17/eboss/photoObj/frames/{row.rerun}/{row.run}/{row.camcol}/frame-{band}-{str(row.run).zfill(6)}-{row.camcol}-{str(row.field).zfill(4)}.fits.bz2\\n\"\n",
    "                )\n",
    "        print(\"Success: Generated URLs\")\n",
    "\n",
    "        with self.output().open(mode=\"w\") as f:\n",
    "            for u in urls:\n",
    "                f.write(u)\n",
    "\n",
    "class ImageDownloader(luigi.Task):\n",
    "    \"\"\"\n",
    "    Downloads all files in specified range of the urls.txt. Usually used with copies of itself\n",
    "    with different parameters to span the whole range of urls to download.\n",
    "\n",
    "    Example::\n",
    "        n_workers = 10  # should be limited by your hardware\n",
    "        n_urls = 50000\n",
    "        chunk = n_urls // n_workers\n",
    "\n",
    "        luigi.build(\n",
    "            [data_downloader.Downloader(lower=i, upper=i + chunk) for i in range(0, n_urls, chunk)],\n",
    "            local_scheduler=True,\n",
    "            workers=n_workers)\n",
    "    \"\"\"\n",
    "    ...\n",
    "    \n",
    "    @classmethod\n",
    "    @contextmanager\n",
    "    def download(cls, url) -> ContextManager:\n",
    "        \"\"\"Context manager for downloading FITS images from the SDSS\n",
    "\n",
    "        :param url: string url to fits file on SDSS server\n",
    "        :return: ContextManager\"\"\"\n",
    "\n",
    "        # Generating temporary and final file names\n",
    "        tmp = f\"{hash(url)}.fits.bz2\"\n",
    "        # skipping the new line character at the end\n",
    "        file_name = url[-31:-1]\n",
    "\n",
    "        try:\n",
    "            # If file already exists, don't download\n",
    "            if os.path.exists(join(\"data\", \"images\", f\"{file_name}\")):\n",
    "                print(\"Skipping!!\", file_name)\n",
    "                yield file_name\n",
    "\n",
    "            else:\n",
    "                # download file to the tmp file name\n",
    "                os.system(f\"wget -O data/images/{tmp} {url}\")\n",
    "                # atomically move/rename the successful download to the final name\n",
    "                os.system(f\"mv data/images/{tmp} data/images/{file_name}\")\n",
    "                yield file_name\n",
    "        finally:\n",
    "\n",
    "            # Cleanup, if tmp still exists\n",
    "            if os.path.exists(tmp):\n",
    "                os.remove(tmp)\n",
    "\n",
    "            if not os.path.exists(join(\"data\", \"images\", f\"{file_name}\")):\n",
    "                # didn't download, try again\n",
    "                print(\"Trying again\")\n",
    "                cls.download(url)\n",
    "\n",
    "    def run(self):\n",
    "        \"\"\"Downloads and moves the images from the SDSS atomically\"\"\"\n",
    "        with self.input().open(\"r\") as urls:\n",
    "            for i, url in enumerate(urls):\n",
    "                # only downloads the section of urls that have been passed in\n",
    "                # as Luigi parameter\n",
    "                if i >= self.lower and i <= self.upper:\n",
    "                    # using our context manager to download the urls\n",
    "                    with self.download(url) as d:\n",
    "                        print(f\"Success: {d}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18736c52",
   "metadata": {},
   "source": [
    "## Preprocessing <a name=\"preprocessing\"></a>\n",
    "\n",
    "After downloading the five images (called FITS images, the file format used in astronomy) for each observation, there was about 80GB of compressed (180GB uncompressed) raw data.\n",
    "<img src='docs/images/preprocess_vis.png' width=\"400\" height=\"400\"> \n",
    "\n",
    "The preprocessing looked like above: reading in the zipped file with the built-n bz2 library, opening the image with astropy’s file handing functions, then determining the pixel grid and sky coordinates from this image using astropy’s WCS module, which uses the metadata in every FITS image to determine exactly what part of the sky each pixel in the image corresponds to.\n",
    "\n",
    "That makes it straightforward to convert from knowing where something is in the sky to where it is in your image. I used that process to center a (32, 32) square on the object in each of the five image frames & crop.\n",
    "\n",
    "There were a number of challenges I ran into in this step:\n",
    "\n",
    "- Despite the safeguards in ImageDownloader, empty files still get saved to disk occasionally, so I would get an OSError when starting the preprocessing step.\n",
    "\n",
    "- There were a few observations in the tabular data that were not downloaded. Either the SDSS didn’t actually have the image corresponding to the tabular data entry or it was stored on a different part of their database. Since this was only a handful of files, I elected to skip them and build in error handling for FileNotFound.\n",
    "\n",
    "I wrapped the preprocessing in a try except for the two errors I expected to occur and log the files that cause those errors.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c35631e5",
   "metadata": {},
   "source": [
    "### Preprocessing Code  <a name=\"preprocessing_code\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "926c2229",
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are the three functions that do all the heavy lifting of the pre-processing\n",
    "\n",
    "def get_data_cube(observation: pd.Series) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Takes in a row of the tabular data corresponding to one observation and returns (5, dim, dim) \n",
    "    datacube of observations\n",
    "    \"\"\"\n",
    "    dim = int(omegaconf.OmegaConf.load(os.path.join(abs_path, \"conf\", \"preprocessing.yaml\"))[\"dim\"])\n",
    "\n",
    "    bands = [\"u\", \"g\", \"r\", \"i\", \"z\"]\n",
    "    files = []\n",
    "    for b in bands:\n",
    "        files.append(\n",
    "            f\"frame-{b}-{str(observation.run).zfill(6)}-{observation.camcol}-{str(observation.field).zfill(4)}.fits.bz2\"\n",
    "        )\n",
    "\n",
    "    data_cube = np.zeros((5, dim, dim), dtype=np.float64)\n",
    "    for i, f in enumerate(files):\n",
    "        data_cube[i] = get_cropped(observation, f, dim)\n",
    "\n",
    "    # datacube should be (5, dim, dim)\n",
    "    return data_cube\n",
    "\n",
    "\n",
    "def get_cropped(observation: pd.Series, fits_file: str, dim: int) -> np.array:\n",
    "    \"\"\"\n",
    "    Given a file name, returns the cropped image as np array centered on the observation\n",
    "    Unzips the file in memory, converts the RA & DEC of obj to pixel coords, centers on object & crops to 32x32.\n",
    "\n",
    "    :param observation: row in tabular data corresponding to observation\n",
    "    :param fits: path to corresponding unzipped fits image\n",
    "    :param dim: int for one side of square image to return (default is 32)\n",
    "    :return: np.array of one cropped image to load into datacube\n",
    "    \"\"\"\n",
    "    # config path loading\n",
    "    local_paths = omegaconf.OmegaConf.load(os.path.join(abs_path, \"conf\", \"local_paths.yaml\"))\n",
    "    fits_file_path = os.path.join(local_paths[\"data\"], local_paths[\"images\"], fits_file)\n",
    "\n",
    "    # reading the compressed file without extracting it\n",
    "    with bz2.BZ2File(fits_file_path, \"rb\") as fits:\n",
    "        # opening the image from the uncompressed read\n",
    "        with astropy.io.fits.open(fits) as hdulist:\n",
    "\n",
    "            # header contains meta data about the image\n",
    "            header = hdulist[0].header\n",
    "            # create world coordinate system from header\n",
    "            wcs = astropy.wcs.WCS(header)\n",
    "            # get a coordinate object of observation\n",
    "            coord = astropy.coordinates.SkyCoord(ra=observation.ra, dec=observation.dec, unit=\"deg\")\n",
    "            # pixel coordinates of center of observation\n",
    "            target = astropy.wcs.utils.skycoord_to_pixel(coord, wcs)\n",
    "            target = (int(target[0]), int(target[1]))\n",
    "            # actual image\n",
    "            data = hdulist[0].data\n",
    "            assert type(dim) == int  # sanity check\n",
    "            # crop the image into (dim, dim) square\n",
    "            cropped = get_square(data, target, dim)\n",
    "    return cropped\n",
    "\n",
    "\n",
    "def get_square(data: np.array, center: tuple, dim: int) -> np.array:\n",
    "    \"\"\"\n",
    "    Crops the FITS image to a (dim, dim) shaped numpy array, centered on\n",
    "    the object of interest.\n",
    "\n",
    "    For more info on the WCS for astropy & how it handles (x, y) math vs array\n",
    "    convention: https://docs.astropy.org/en/stable/wcs/index.html\n",
    "    https://docs.astropy.org/en/stable/api/astropy.wcs.utils.skycoord_to_pixel.html\n",
    "\n",
    "    :param data: np.array of the actual image itself\n",
    "    :param center: (columns, rows) - (x, y) in math perspective\n",
    "    :param dim: int for square dimensions to return\n",
    "\n",
    "    :return: np.array of shape: (dim, dim). Is the cropped image.\n",
    "    \"\"\"\n",
    "    center = list(center)\n",
    "\n",
    "    # Center[0] is column wise, so shape[1]\n",
    "    if center[0] - dim // 2 < 0:\n",
    "        center[0] = dim // 2\n",
    "    if center[0] + dim // 2 > data.shape[1]:\n",
    "        center[0] = data.shape[0]\n",
    "\n",
    "    # Center[1] is row wise, so shape[0]\n",
    "    if center[1] - dim // 2 < 0:\n",
    "        center[1] = dim // 2\n",
    "    if center[1] + dim // 2 > data.shape[0]:\n",
    "        center[1] = data.shape[0]\n",
    "\n",
    "    return data[center[1] - dim // 2 : center[1] + dim // 2, center[0] - dim // 2 : center[0] + dim // 2].copy()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac2611f8",
   "metadata": {},
   "source": [
    "## Data EDA <a name=\"eda\"></a>\n",
    "I ended up with 80GB *compressed* raw data (240GB uncompressed). I had to be careful to iterate over the raw data and ensure that there were no unzipped artifacts laying around that would potentially crash my hard drive. \n",
    "\n",
    "After the data pre-processing, I ended up with just under 7,800 training points (32x32x5 data cubes) which was only 40MB! The data generator step ultimately wasn't needed at this smaller dataset size, but I still had implemented and tested it, so I continued to use it since it let me also do multiprocessing for the training. \n",
    "\n",
    "<img src='docs/images/zdist.png' width=\"500\" height=\"400\">\n",
    "\n",
    "This is the distribution of redshifts (often just labelled \"z\", which is confusing because there is also a z-band for photometry as well) for the dataset, which matches up pretty well with the general distribution of SDSS redshifts in general.\n",
    "\n",
    "### Preprocessed Data: Visualized <a name=\"eda_vis\"></a>\n",
    "An example pre-processed image! Note that the composite image is not part of the data pipeline, it's just to get a better feel of the relative intensities in the image across the bands. Also, it's only a three channel image, even though our data would be \"five\" channels.\n",
    "\n",
    "Blue-Green-Red Composite   |  U-band\n",
    ":-------------------------:|:-------------------------:\n",
    "![](docs/images/composite_ugr.png)  |  ![](docs/images/preprocess1.png)\n",
    "\n",
    "G-band                     |  R-band\n",
    ":-------------------------:|:-------------------------:\n",
    "![](docs/images/preprocess2.png)  |  ![](docs/images/preprocess3.png)\n",
    "\n",
    "I-band                     |  Z-band\n",
    ":-------------------------:|:-------------------------:\n",
    "![](docs/images/preprocess4.png)  |  ![](docs/images/preprocess5.png)\n",
    "\n",
    "We can see that it did successfully crop on each object!\n",
    "\n",
    "### EDA Stats Visualized <a name=\"eda_stats\"></a>\n",
    "Calculated from the visualized statistical datacubes for reference:\n",
    "<img src='docs/images/stats.png' width=\"400\" height=\"300\">\n",
    "\n",
    "Mean Images across bands   |  .\n",
    ":-------------------------:|:-------------------------:\n",
    "![](docs/images/mean_u.png)  |  ![](docs/images/mean_u.png)\n",
    "![](docs/images/mean_g.png)  |  ![](docs/images/mean_r.png)\n",
    "![](docs/images/mean_i.png)  |  ![](docs/images/mean_z.png)\n",
    "\n",
    "The mean images were helpful to sanity check that the preprocessing aligning went smoothly.\n",
    "\n",
    "Std Images across bands    |  .\n",
    ":-------------------------:|:-------------------------:\n",
    "![](docs/images/std_u.png)  |  ![](docs/images/std_u.png)\n",
    "![](docs/images/std_g.png)  |  ![](docs/images/std_r.png)\n",
    "![](docs/images/std_i.png)  |  ![](docs/images/std_z.png)\n",
    "\n",
    "An interesting thing about the std are the very bright blobs on the fringes of the image. I guessed that this would be due to some images having bright other objects in the frame. I checked this by looking at the maximum images across the different bands and found an almost identical max image across all five bands:\n",
    "\n",
    "<img src='docs/images/max_i.png' width=\"400\" height=\"300\">\n",
    "\n",
    "Notice how the center of the image is comparatively dim. I think that the blobs that match up perfectly with the std deviation images were so saturated that they skewed the entire dataset for those observations. Presumably there are n-blobs worth of images with saturated extra objects that contributed to this max pixel image."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c871115d",
   "metadata": {},
   "source": [
    "# Architecture <a name=\"architecture\"></a>\n",
    "\n",
    "The base for this was the Pasquet et al paper [2], whose associated code repository can be found here: https://github.com/umesh-timalsina/redshift/.\n",
    "They don't explicitly state what their loss function was in their paper or their optimizer or actually what their smaller architecture was, just that they did work on one for 10k training points. \n",
    "\n",
    "Their Architecture         |  My Final Model\n",
    ":-------------------------:|:-------------------------:\n",
    "![](docs/images/model.png) |  ![](docs/images/flipmodel.png)\n",
    "\n",
    "#### Differences: \n",
    "Most notably, I added normalization and data-augmentation pre-processing layers and have one less dense layer (with potentially problematic ReLu activation) and two fewer inception module layers. They were also using a parametrized ReLu activation, which I switched to a LeakyReLu when I suspected that part of the extremely fast convergence might be due to a dying ReLU (I suspect) issue.\n",
    "\n",
    "While the random flip layer doesn't generate any copies of the training data as \"extra\" training points with flips as regular data augmentation, it does effectively allow you to pretend that you do have more data. With each epoch, instead of seeing every single data point, you can only see 1/4 of all the possible images in the training set, as each image could be randomly flipped horizonally, vertically, or both. \n",
    "\n",
    "Since we employ early stopping (100 epochs, but always converges in under 50), it does let the model go over the data as much as it wants.\n",
    "\n",
    "#### Why Average Pooling?\n",
    "Also to note: Pasquet says in the paper that they use average pooling instead of max because of the amount of noise from the sky background in most images.\n",
    "\n",
    "It's very common in astronomy to deal with cosmic rays and other bright sources of noise that will saturate the camera. Usually during the data cleaning and noise subtraction steps done in the catalog data pipelines, you use the median image across observations for the night to help remove these saturating events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a3ba5c59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_4 (InputLayer)           [(None, 32, 32, 5)]  0           []                               \n",
      "                                                                                                  \n",
      " normalization_3 (Normalization  (None, 32, 32, 5)   0           ['input_4[0][0]']                \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " random_flip_3 (RandomFlip)     (None, 32, 32, 5)    0           ['normalization_3[0][0]']        \n",
      "                                                                                                  \n",
      " conv2d_51 (Conv2D)             (None, 32, 32, 64)   8064        ['random_flip_3[0][0]']          \n",
      "                                                                                                  \n",
      " average_pooling2d_18 (AverageP  (None, 16, 16, 64)  0           ['conv2d_51[0][0]']              \n",
      " ooling2D)                                                                                        \n",
      "                                                                                                  \n",
      " conv2d_52 (Conv2D)             (None, 16, 16, 48)   3120        ['average_pooling2d_18[0][0]']   \n",
      "                                                                                                  \n",
      " conv2d_53 (Conv2D)             (None, 16, 16, 48)   3120        ['average_pooling2d_18[0][0]']   \n",
      "                                                                                                  \n",
      " conv2d_54 (Conv2D)             (None, 16, 16, 48)   3120        ['average_pooling2d_18[0][0]']   \n",
      "                                                                                                  \n",
      " conv2d_55 (Conv2D)             (None, 16, 16, 64)   4160        ['average_pooling2d_18[0][0]']   \n",
      "                                                                                                  \n",
      " conv2d_56 (Conv2D)             (None, 16, 16, 64)   27712       ['conv2d_52[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_57 (Conv2D)             (None, 16, 16, 64)   76864       ['conv2d_53[0][0]']              \n",
      "                                                                                                  \n",
      " average_pooling2d_19 (AverageP  (None, 16, 16, 48)  0           ['conv2d_54[0][0]']              \n",
      " ooling2D)                                                                                        \n",
      "                                                                                                  \n",
      " concatenate_9 (Concatenate)    (None, 16, 16, 240)  0           ['conv2d_55[0][0]',              \n",
      "                                                                  'conv2d_56[0][0]',              \n",
      "                                                                  'conv2d_57[0][0]',              \n",
      "                                                                  'average_pooling2d_19[0][0]']   \n",
      "                                                                                                  \n",
      " average_pooling2d_20 (AverageP  (None, 8, 8, 240)   0           ['concatenate_9[0][0]']          \n",
      " ooling2D)                                                                                        \n",
      "                                                                                                  \n",
      " conv2d_58 (Conv2D)             (None, 8, 8, 92)     22172       ['average_pooling2d_20[0][0]']   \n",
      "                                                                                                  \n",
      " conv2d_59 (Conv2D)             (None, 8, 8, 92)     22172       ['average_pooling2d_20[0][0]']   \n",
      "                                                                                                  \n",
      " conv2d_60 (Conv2D)             (None, 8, 8, 92)     22172       ['average_pooling2d_20[0][0]']   \n",
      "                                                                                                  \n",
      " conv2d_61 (Conv2D)             (None, 8, 8, 128)    30848       ['average_pooling2d_20[0][0]']   \n",
      "                                                                                                  \n",
      " conv2d_62 (Conv2D)             (None, 8, 8, 128)    106112      ['conv2d_58[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_63 (Conv2D)             (None, 8, 8, 128)    294528      ['conv2d_59[0][0]']              \n",
      "                                                                                                  \n",
      " average_pooling2d_21 (AverageP  (None, 8, 8, 92)    0           ['conv2d_60[0][0]']              \n",
      " ooling2D)                                                                                        \n",
      "                                                                                                  \n",
      " concatenate_10 (Concatenate)   (None, 8, 8, 476)    0           ['conv2d_61[0][0]',              \n",
      "                                                                  'conv2d_62[0][0]',              \n",
      "                                                                  'conv2d_63[0][0]',              \n",
      "                                                                  'average_pooling2d_21[0][0]']   \n",
      "                                                                                                  \n",
      " average_pooling2d_22 (AverageP  (None, 4, 4, 476)   0           ['concatenate_10[0][0]']         \n",
      " ooling2D)                                                                                        \n",
      "                                                                                                  \n",
      " conv2d_64 (Conv2D)             (None, 4, 4, 92)     43884       ['average_pooling2d_22[0][0]']   \n",
      "                                                                                                  \n",
      " conv2d_65 (Conv2D)             (None, 4, 4, 92)     43884       ['average_pooling2d_22[0][0]']   \n",
      "                                                                                                  \n",
      " conv2d_66 (Conv2D)             (None, 4, 4, 128)    61056       ['average_pooling2d_22[0][0]']   \n",
      "                                                                                                  \n",
      " conv2d_67 (Conv2D)             (None, 4, 4, 128)    106112      ['conv2d_64[0][0]']              \n",
      "                                                                                                  \n",
      " average_pooling2d_23 (AverageP  (None, 4, 4, 92)    0           ['conv2d_65[0][0]']              \n",
      " ooling2D)                                                                                        \n",
      "                                                                                                  \n",
      " concatenate_11 (Concatenate)   (None, 4, 4, 348)    0           ['conv2d_66[0][0]',              \n",
      "                                                                  'conv2d_67[0][0]',              \n",
      "                                                                  'average_pooling2d_23[0][0]']   \n",
      "                                                                                                  \n",
      " flatten_3 (Flatten)            (None, 5568)         0           ['concatenate_11[0][0]']         \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 100)          556900      ['flatten_3[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 1,436,000\n",
      "Trainable params: 1,436,000\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# The final architecture used:\n",
    "model.model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8383be50",
   "metadata": {},
   "source": [
    "### Architecture Code <a name=\"arch_code\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b4c5381",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class BaseModel:\n",
    "    \"\"\"\n",
    "    Base class for Model for this pipeline. Uses lazy-loading for most heavy-lifting attributes\n",
    "\n",
    "    :param batch_size: int - specified via config file\n",
    "    :param test_split: float - specified via config file\n",
    "    :param num_classes: int - specified via config file\n",
    "    :param lr: float - specified via config file\n",
    "    :param seed: int - specified via config file\n",
    "    :param input_img_shape: tuple - specified via config file\n",
    "    \"\"\"\n",
    "\n",
    "    batch_size: int\n",
    "    test_split: float\n",
    "    num_classes: int\n",
    "    seed: int\n",
    "    lr: float\n",
    "    input_img_shape: tuple\n",
    "    epochs: int\n",
    "\n",
    "    @cached_property\n",
    "    def labels(self) -> dict:\n",
    "        \"\"\"\n",
    "        Creates dict of series object, with objID as the index for easy partition access to\n",
    "        labels.\n",
    "        :return: dict partitioned by 'test' and 'train' of pandas Series with labels\n",
    "        \"\"\"\n",
    "        tabular_name = OmegaConf.load(join(abs_path, \"conf\", \"aws_paths.yaml\"))[\"tabular_data\"]\n",
    "        tabular_path = join(abs_path, \"..\", \"data\", tabular_name)\n",
    "        df = pd.read_csv(tabular_path, usecols=[\"objID\", \"z\"])\n",
    "\n",
    "        series = pd.Series(data=df.z.values, index=df.objID, name=\"z\")\n",
    "\n",
    "        return {\"test\": series.loc[self.partition[\"test\"]], \"train\": series.loc[self.partition[\"train\"]]}\n",
    "\n",
    "    @cached_property\n",
    "    def partition(self) -> dict:\n",
    "        \"\"\"\n",
    "        Creates the partition for testing & training by object ID.\n",
    "\n",
    "        This is used by labels to split the labels into test & train sets and also by DataGenerator to\n",
    "        create batches, etc.\n",
    "\n",
    "        :return: dict of string file names partitioned by 'test' and 'train'\n",
    "        \"\"\"\n",
    "        # get a numpy array of all the processed data ready for training\n",
    "        dataset = glob.glob(join(\"data\", \"processed\", \"*.npy\"))\n",
    "        func = lambda file: int(file[15:-15])\n",
    "        dataset = np.array(list(map(func, dataset)))\n",
    "\n",
    "        # this is a numpy array of integers, which are the object ID's. Each processed datacube\n",
    "        # is named with its object ID\n",
    "        unique_data = np.unique(dataset)\n",
    "\n",
    "        # random shuffle\n",
    "        np.random.shuffle(unique_data)\n",
    "\n",
    "        partition = {}\n",
    "\n",
    "        # the test/train split, note the hard to spot colons for slicing\n",
    "        partition[\"test\"] = unique_data[: int(len(unique_data) * self.test_split)]\n",
    "        partition[\"train\"] = unique_data[int(len(unique_data) * self.test_split) :]\n",
    "\n",
    "        return partition\n",
    "\n",
    "    @cached_property\n",
    "    def training_generator(self) -> DataGenerator:\n",
    "        \"\"\"\n",
    "        Cached property, so only gets called once when the property is accessed. Part of BaseModel class.\n",
    "        Creates an intance of the DataGenerator class with the relevant test/train partition to use for training\n",
    "        or prediction.\n",
    "        \"\"\"\n",
    "        return DataGenerator(self.partition[\"train\"], self.labels[\"train\"], self.num_classes, batch_size=self.batch_size)  # , **params)\n",
    "\n",
    "    @cached_property\n",
    "    def testing_generator(self) -> DataGenerator:\n",
    "        \"\"\"\n",
    "        Cached property, so only gets called once when the property is accessed. Part of BaseModel class.\n",
    "        Creates an intance of the DataGenerator class with the relevant test/train partition to use for training\n",
    "        or prediction.\n",
    "        \"\"\"\n",
    "        return DataGenerator(self.partition[\"test\"], self.labels[\"test\"], self.num_classes, batch_size=self.batch_size)  # , **params)\n",
    "\n",
    "    def train(self, checkpoint_path, history_path, workers=12):\n",
    "        \"\"\"\n",
    "        Wrapper for fitting the model with the generator we created\n",
    "        Note we can only use multiprocessing because we're passing in a generator\n",
    "\n",
    "        :param checkpoint_path: str\n",
    "        :param history_path: str\n",
    "        :param workers: int\n",
    "        \"\"\"\n",
    "        my_callbacks = [\n",
    "            tf.keras.callbacks.CSVLogger(history_path, separator=\",\", append=True),\n",
    "            tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path, verbose=1, save_best_only=True),\n",
    "            tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", min_delta=0.0001, patience=3, verbose=1, mode=\"auto\"),\n",
    "        ]\n",
    "\n",
    "        history = self.model.fit(\n",
    "            self.training_generator,\n",
    "            validation_data=self.testing_generator,\n",
    "            epochs=epochs,\n",
    "            use_multiprocessing=True,\n",
    "            callbacks=my_callbacks,\n",
    "            workers=workers\n",
    "        )\n",
    "        return history\n",
    "\n",
    "    def loss(self, true, pred):\n",
    "        \"\"\"\n",
    "        Custom loss function for keras\n",
    "        True should be a float, pred should be a (num_classes,) shaped array.\n",
    "        Calculates the MSE between the true value and the expected value of the output\n",
    "        \"\"\"\n",
    "        return tf.square(tf.subtract(true, tf.reduce_sum(tf.multiply(self.bin_values, pred))))\n",
    "\n",
    "    @cached_property\n",
    "    def bin_values(self):\n",
    "        \"\"\"returns array with the same length as the number of classes, with the midpoint of the start and end range\n",
    "        stored (ie, the bin containing redshift values 0 to 0.2 will have bin value 0.1)\n",
    "        \"\"\"\n",
    "        max_val = 0.4  # since 0.39.. is the max redshift value that we see\n",
    "        bins = np.linspace(0, max_val, self.num_classes, endpoint=False, dtype=np.float32)\n",
    "        bin_values = bins + bins[1] / 2\n",
    "        return bin_values\n",
    "\n",
    "\n",
    "class InceptionModel(BaseModel):\n",
    "    \"\"\"\n",
    "    Inception model from Pasquet et al paper. Wrapper for many tensorflow/keras functions.\n",
    "\n",
    "    :param batch_size: int - specified via config file\n",
    "    :param test_split: float - specified via config file\n",
    "    :param num_classes: int - specified via config file\n",
    "    :param lr: float - specified via config file\n",
    "    :param seed: int - specified via config file\n",
    "    :param input_img_shape: tuple - specified via config file\n",
    "    \"\"\"\n",
    "    @cached_property\n",
    "    def model(self):\n",
    "        \"\"\"\n",
    "        Constructs the model architecture. Most of the keras code here is all from the Inception model\n",
    "        in https://github.com/umesh-timalsina/redshift/blob/3c11608d5818ae58ab2ce084832b56766858b3a1/model/model.py.\n",
    "        Which is the code repository for the Pasquet et al 2018 paper\n",
    "        \"\"\"\n",
    "        # Input Layer Galactic Images\n",
    "        image_input = Input(shape=self.input_img_shape[1:])\n",
    "\n",
    "        # Adding a preprocessing layer for normalization\n",
    "        norm = Normalization(mean=np.load('data/stats/mean.npy'), variance=np.load(\"data/stats/variance.npy\"), axis=(1,2,3))\n",
    "        norm_out = norm(image_input)\n",
    "\n",
    "        # adding a random flip\n",
    "        flip = RandomFlip()\n",
    "        flip_out = flip(norm_out)\n",
    "\n",
    "        # Back to their code\n",
    "        # Convolution Layer 1\n",
    "        conv_1 = Conv2D(64, kernel_size=(5, 5), padding=\"same\", activation=LeakyReLU())\n",
    "        conv_1_out = conv_1(flip_out)\n",
    "\n",
    "        # Pooling Layer 1\n",
    "        pooling_layer1 = AveragePooling2D(pool_size=(2, 2), strides=2, padding=\"same\")\n",
    "        pooling_layer1_out = pooling_layer1(conv_1_out)\n",
    "\n",
    "        # Inception Layer 1\n",
    "        inception_layer1_out = self.add_inception_layer(pooling_layer1_out, num_f1=48, num_f2=64)\n",
    "\n",
    "        # Pooling Layer 2\n",
    "        pooling_layer2 = AveragePooling2D(pool_size=(2, 2), strides=2, padding=\"same\")\n",
    "        pooling_layer2_out = pooling_layer2(inception_layer1_out) # changed from 2\n",
    "\n",
    "        # Inception Layer 3\n",
    "        inception_layer3_out = self.add_inception_layer(pooling_layer2_out, 92, 128)\n",
    "\n",
    "        # Pooling Layer 3\n",
    "        pooling_layer3 = AveragePooling2D(pool_size=(2, 2), strides=2, padding=\"same\")\n",
    "        pooling_layer3_out = pooling_layer3(inception_layer3_out)  # modified 4 -> 3\n",
    "\n",
    "        # Inception Layer 5\n",
    "        inception_layer5_out = self.add_inception_layer(pooling_layer3_out, 92, 128, kernel_5=False)\n",
    "\n",
    "        # input_to_pooling = cur_inception_in\n",
    "        input_to_dense = Flatten(data_format=\"channels_last\")(inception_layer5_out)\n",
    "        model_output = Dense(units=self.num_classes, activation=\"softmax\")(input_to_dense)\n",
    "\n",
    "        ### Construction, summary, and compiling\n",
    "        model = tensorflow.keras.Model(inputs=[image_input], outputs=model_output)\n",
    "\n",
    "        model.summary()\n",
    "        opt = Adam(learning_rate=self.lr, epsilon=self.epsilon)\n",
    "        model.compile(optimizer=opt, loss=self.loss)\n",
    "        return model\n",
    "\n",
    "    def add_inception_layer(self, input_weights, num_f1, num_f2, kernel_5=True):\n",
    "        \"\"\"\n",
    "        These convolutional layers take care of the inception layer.\n",
    "        This function is also not my code, also directly from the Pasquet et al paper repo.\n",
    "        \"\"\"\n",
    "        # Conv Layer 1 and Layer 2: Feed them to convolution layers 5 and 6\n",
    "        c1 = Conv2D(num_f1, kernel_size=(1, 1), padding=\"same\", activation=LeakyReLU())\n",
    "        c1_out = c1(input_weights)\n",
    "        if kernel_5:\n",
    "            c2 = Conv2D(num_f1, kernel_size=(1, 1), padding=\"same\", activation=LeakyReLU())\n",
    "            c2_out = c2(input_weights)\n",
    "\n",
    "        # Conv Layer 3 : Feed to pooling layer 1\n",
    "        c3 = Conv2D(num_f1, kernel_size=(1, 1), padding=\"same\", activation=LeakyReLU())\n",
    "        c3_out = c3(input_weights)\n",
    "\n",
    "        # Conv Layer 4: Feed directly to concat\n",
    "        c4 = Conv2D(num_f2, kernel_size=(1, 1), padding=\"same\", activation=LeakyReLU())\n",
    "        c4_out = c4(input_weights)\n",
    "\n",
    "        # Conv Layer 5: Feed from c1, feed to concat\n",
    "        c5 = Conv2D(num_f2, kernel_size=(3, 3), padding=\"same\", activation=LeakyReLU())\n",
    "        c5_out = c5(c1_out)\n",
    "\n",
    "        # Conv Layer 6: Feed from c2, feed to concat\n",
    "        if kernel_5:\n",
    "            c6 = Conv2D(num_f2, kernel_size=(5, 5), padding=\"same\", activation=LeakyReLU())\n",
    "            c6_out = c6(c2_out)\n",
    "\n",
    "        # Pooling Layer 1: Feed from conv3, feed to concat\n",
    "        p1 = AveragePooling2D(pool_size=(2, 2), strides=1, padding=\"same\")\n",
    "        p1_out = p1(c3_out)\n",
    "\n",
    "        if kernel_5:\n",
    "            return concatenate([c4_out, c5_out, c6_out, p1_out])\n",
    "        else:\n",
    "            return concatenate([c4_out, c5_out, p1_out])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4a7b0c5",
   "metadata": {},
   "source": [
    "### Training Example <a name=\"train_code\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "4d1116c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_4\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_5 (InputLayer)           [(None, 32, 32, 5)]  0           []                               \n",
      "                                                                                                  \n",
      " normalization_4 (Normalization  (None, 32, 32, 5)   0           ['input_5[0][0]']                \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " random_flip_4 (RandomFlip)     (None, 32, 32, 5)    0           ['normalization_4[0][0]']        \n",
      "                                                                                                  \n",
      " conv2d_68 (Conv2D)             (None, 32, 32, 64)   8064        ['random_flip_4[0][0]']          \n",
      "                                                                                                  \n",
      " average_pooling2d_24 (AverageP  (None, 16, 16, 64)  0           ['conv2d_68[0][0]']              \n",
      " ooling2D)                                                                                        \n",
      "                                                                                                  \n",
      " conv2d_69 (Conv2D)             (None, 16, 16, 48)   3120        ['average_pooling2d_24[0][0]']   \n",
      "                                                                                                  \n",
      " conv2d_70 (Conv2D)             (None, 16, 16, 48)   3120        ['average_pooling2d_24[0][0]']   \n",
      "                                                                                                  \n",
      " conv2d_71 (Conv2D)             (None, 16, 16, 48)   3120        ['average_pooling2d_24[0][0]']   \n",
      "                                                                                                  \n",
      " conv2d_72 (Conv2D)             (None, 16, 16, 64)   4160        ['average_pooling2d_24[0][0]']   \n",
      "                                                                                                  \n",
      " conv2d_73 (Conv2D)             (None, 16, 16, 64)   27712       ['conv2d_69[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_74 (Conv2D)             (None, 16, 16, 64)   76864       ['conv2d_70[0][0]']              \n",
      "                                                                                                  \n",
      " average_pooling2d_25 (AverageP  (None, 16, 16, 48)  0           ['conv2d_71[0][0]']              \n",
      " ooling2D)                                                                                        \n",
      "                                                                                                  \n",
      " concatenate_12 (Concatenate)   (None, 16, 16, 240)  0           ['conv2d_72[0][0]',              \n",
      "                                                                  'conv2d_73[0][0]',              \n",
      "                                                                  'conv2d_74[0][0]',              \n",
      "                                                                  'average_pooling2d_25[0][0]']   \n",
      "                                                                                                  \n",
      " average_pooling2d_26 (AverageP  (None, 8, 8, 240)   0           ['concatenate_12[0][0]']         \n",
      " ooling2D)                                                                                        \n",
      "                                                                                                  \n",
      " conv2d_75 (Conv2D)             (None, 8, 8, 92)     22172       ['average_pooling2d_26[0][0]']   \n",
      "                                                                                                  \n",
      " conv2d_76 (Conv2D)             (None, 8, 8, 92)     22172       ['average_pooling2d_26[0][0]']   \n",
      "                                                                                                  \n",
      " conv2d_77 (Conv2D)             (None, 8, 8, 92)     22172       ['average_pooling2d_26[0][0]']   \n",
      "                                                                                                  \n",
      " conv2d_78 (Conv2D)             (None, 8, 8, 128)    30848       ['average_pooling2d_26[0][0]']   \n",
      "                                                                                                  \n",
      " conv2d_79 (Conv2D)             (None, 8, 8, 128)    106112      ['conv2d_75[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_80 (Conv2D)             (None, 8, 8, 128)    294528      ['conv2d_76[0][0]']              \n",
      "                                                                                                  \n",
      " average_pooling2d_27 (AverageP  (None, 8, 8, 92)    0           ['conv2d_77[0][0]']              \n",
      " ooling2D)                                                                                        \n",
      "                                                                                                  \n",
      " concatenate_13 (Concatenate)   (None, 8, 8, 476)    0           ['conv2d_78[0][0]',              \n",
      "                                                                  'conv2d_79[0][0]',              \n",
      "                                                                  'conv2d_80[0][0]',              \n",
      "                                                                  'average_pooling2d_27[0][0]']   \n",
      "                                                                                                  \n",
      " average_pooling2d_28 (AverageP  (None, 4, 4, 476)   0           ['concatenate_13[0][0]']         \n",
      " ooling2D)                                                                                        \n",
      "                                                                                                  \n",
      " conv2d_81 (Conv2D)             (None, 4, 4, 92)     43884       ['average_pooling2d_28[0][0]']   \n",
      "                                                                                                  \n",
      " conv2d_82 (Conv2D)             (None, 4, 4, 92)     43884       ['average_pooling2d_28[0][0]']   \n",
      "                                                                                                  \n",
      " conv2d_83 (Conv2D)             (None, 4, 4, 128)    61056       ['average_pooling2d_28[0][0]']   \n",
      "                                                                                                  \n",
      " conv2d_84 (Conv2D)             (None, 4, 4, 128)    106112      ['conv2d_81[0][0]']              \n",
      "                                                                                                  \n",
      " average_pooling2d_29 (AverageP  (None, 4, 4, 92)    0           ['conv2d_82[0][0]']              \n",
      " ooling2D)                                                                                        \n",
      "                                                                                                  \n",
      " concatenate_14 (Concatenate)   (None, 4, 4, 348)    0           ['conv2d_83[0][0]',              \n",
      "                                                                  'conv2d_84[0][0]',              \n",
      "                                                                  'average_pooling2d_29[0][0]']   \n",
      "                                                                                                  \n",
      " flatten_4 (Flatten)            (None, 5568)         0           ['concatenate_14[0][0]']         \n",
      "                                                                                                  \n",
      " dense_4 (Dense)                (None, 100)          556900      ['flatten_4[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 1,436,000\n",
      "Trainable params: 1,436,000\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Leftover:  20\n",
      "Leftover:  13\n",
      "Epoch 1/50\n",
      "193/193 [==============================] - ETA: 0s - loss: 22.0817\n",
      "Epoch 1: val_loss improved from inf to 0.19475, saving model to data/models/demo2345_checkpoint\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as leaky_re_lu_68_layer_call_fn, leaky_re_lu_68_layer_call_and_return_conditional_losses, leaky_re_lu_69_layer_call_fn, leaky_re_lu_69_layer_call_and_return_conditional_losses, leaky_re_lu_70_layer_call_fn while saving (showing 5 of 34). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: data/models/demo2345_checkpoint/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: data/models/demo2345_checkpoint/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "193/193 [==============================] - 22s 105ms/step - loss: 22.0817 - val_loss: 0.1948\n",
      "Epoch 2/50\n",
      "193/193 [==============================] - ETA: 0s - loss: 0.0370\n",
      "Epoch 2: val_loss improved from 0.19475 to 0.01936, saving model to data/models/demo2345_checkpoint\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as leaky_re_lu_68_layer_call_fn, leaky_re_lu_68_layer_call_and_return_conditional_losses, leaky_re_lu_69_layer_call_fn, leaky_re_lu_69_layer_call_and_return_conditional_losses, leaky_re_lu_70_layer_call_fn while saving (showing 5 of 34). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: data/models/demo2345_checkpoint/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: data/models/demo2345_checkpoint/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "193/193 [==============================] - 23s 113ms/step - loss: 0.0370 - val_loss: 0.0194\n",
      "Epoch 3/50\n",
      "193/193 [==============================] - ETA: 0s - loss: 0.0165\n",
      "Epoch 3: val_loss improved from 0.01936 to 0.01472, saving model to data/models/demo2345_checkpoint\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as leaky_re_lu_68_layer_call_fn, leaky_re_lu_68_layer_call_and_return_conditional_losses, leaky_re_lu_69_layer_call_fn, leaky_re_lu_69_layer_call_and_return_conditional_losses, leaky_re_lu_70_layer_call_fn while saving (showing 5 of 34). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: data/models/demo2345_checkpoint/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: data/models/demo2345_checkpoint/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "193/193 [==============================] - 24s 121ms/step - loss: 0.0165 - val_loss: 0.0147\n",
      "Epoch 4/50\n",
      "193/193 [==============================] - ETA: 0s - loss: 0.0137\n",
      "Epoch 4: val_loss improved from 0.01472 to 0.01290, saving model to data/models/demo2345_checkpoint\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as leaky_re_lu_68_layer_call_fn, leaky_re_lu_68_layer_call_and_return_conditional_losses, leaky_re_lu_69_layer_call_fn, leaky_re_lu_69_layer_call_and_return_conditional_losses, leaky_re_lu_70_layer_call_fn while saving (showing 5 of 34). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: data/models/demo2345_checkpoint/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: data/models/demo2345_checkpoint/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "193/193 [==============================] - 24s 118ms/step - loss: 0.0137 - val_loss: 0.0129\n",
      "Epoch 5/50\n",
      "193/193 [==============================] - ETA: 0s - loss: 0.0124\n",
      "Epoch 5: val_loss improved from 0.01290 to 0.01194, saving model to data/models/demo2345_checkpoint\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as leaky_re_lu_68_layer_call_fn, leaky_re_lu_68_layer_call_and_return_conditional_losses, leaky_re_lu_69_layer_call_fn, leaky_re_lu_69_layer_call_and_return_conditional_losses, leaky_re_lu_70_layer_call_fn while saving (showing 5 of 34). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: data/models/demo2345_checkpoint/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: data/models/demo2345_checkpoint/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "193/193 [==============================] - 23s 117ms/step - loss: 0.0124 - val_loss: 0.0119\n",
      "Epoch 6/50\n",
      "193/193 [==============================] - ETA: 0s - loss: 0.0117\n",
      "Epoch 6: val_loss improved from 0.01194 to 0.01137, saving model to data/models/demo2345_checkpoint\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as leaky_re_lu_68_layer_call_fn, leaky_re_lu_68_layer_call_and_return_conditional_losses, leaky_re_lu_69_layer_call_fn, leaky_re_lu_69_layer_call_and_return_conditional_losses, leaky_re_lu_70_layer_call_fn while saving (showing 5 of 34). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: data/models/demo2345_checkpoint/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: data/models/demo2345_checkpoint/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "193/193 [==============================] - 24s 120ms/step - loss: 0.0117 - val_loss: 0.0114\n",
      "Epoch 7/50\n",
      "193/193 [==============================] - ETA: 0s - loss: 0.0113\n",
      "Epoch 7: val_loss improved from 0.01137 to 0.01098, saving model to data/models/demo2345_checkpoint\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as leaky_re_lu_68_layer_call_fn, leaky_re_lu_68_layer_call_and_return_conditional_losses, leaky_re_lu_69_layer_call_fn, leaky_re_lu_69_layer_call_and_return_conditional_losses, leaky_re_lu_70_layer_call_fn while saving (showing 5 of 34). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: data/models/demo2345_checkpoint/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: data/models/demo2345_checkpoint/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "193/193 [==============================] - 23s 117ms/step - loss: 0.0113 - val_loss: 0.0110\n",
      "Epoch 8/50\n",
      "193/193 [==============================] - ETA: 0s - loss: 0.0110\n",
      "Epoch 8: val_loss improved from 0.01098 to 0.01072, saving model to data/models/demo2345_checkpoint\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as leaky_re_lu_68_layer_call_fn, leaky_re_lu_68_layer_call_and_return_conditional_losses, leaky_re_lu_69_layer_call_fn, leaky_re_lu_69_layer_call_and_return_conditional_losses, leaky_re_lu_70_layer_call_fn while saving (showing 5 of 34). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: data/models/demo2345_checkpoint/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: data/models/demo2345_checkpoint/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "193/193 [==============================] - 24s 119ms/step - loss: 0.0110 - val_loss: 0.0107\n",
      "Epoch 9/50\n",
      "193/193 [==============================] - ETA: 0s - loss: 0.0108\n",
      "Epoch 9: val_loss improved from 0.01072 to 0.01052, saving model to data/models/demo2345_checkpoint\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as leaky_re_lu_68_layer_call_fn, leaky_re_lu_68_layer_call_and_return_conditional_losses, leaky_re_lu_69_layer_call_fn, leaky_re_lu_69_layer_call_and_return_conditional_losses, leaky_re_lu_70_layer_call_fn while saving (showing 5 of 34). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: data/models/demo2345_checkpoint/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: data/models/demo2345_checkpoint/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "193/193 [==============================] - 23s 116ms/step - loss: 0.0108 - val_loss: 0.0105\n",
      "Epoch 10/50\n",
      "193/193 [==============================] - ETA: 0s - loss: 0.0106\n",
      "Epoch 10: val_loss improved from 0.01052 to 0.01037, saving model to data/models/demo2345_checkpoint\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as leaky_re_lu_68_layer_call_fn, leaky_re_lu_68_layer_call_and_return_conditional_losses, leaky_re_lu_69_layer_call_fn, leaky_re_lu_69_layer_call_and_return_conditional_losses, leaky_re_lu_70_layer_call_fn while saving (showing 5 of 34). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: data/models/demo2345_checkpoint/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: data/models/demo2345_checkpoint/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "193/193 [==============================] - 24s 118ms/step - loss: 0.0106 - val_loss: 0.0104\n",
      "Epoch 11/50\n",
      "193/193 [==============================] - ETA: 0s - loss: 0.0105\n",
      "Epoch 11: val_loss improved from 0.01037 to 0.01025, saving model to data/models/demo2345_checkpoint\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as leaky_re_lu_68_layer_call_fn, leaky_re_lu_68_layer_call_and_return_conditional_losses, leaky_re_lu_69_layer_call_fn, leaky_re_lu_69_layer_call_and_return_conditional_losses, leaky_re_lu_70_layer_call_fn while saving (showing 5 of 34). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: data/models/demo2345_checkpoint/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: data/models/demo2345_checkpoint/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "193/193 [==============================] - 24s 120ms/step - loss: 0.0105 - val_loss: 0.0102\n",
      "Epoch 12/50\n",
      "193/193 [==============================] - ETA: 0s - loss: 0.0104\n",
      "Epoch 12: val_loss improved from 0.01025 to 0.01015, saving model to data/models/demo2345_checkpoint\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as leaky_re_lu_68_layer_call_fn, leaky_re_lu_68_layer_call_and_return_conditional_losses, leaky_re_lu_69_layer_call_fn, leaky_re_lu_69_layer_call_and_return_conditional_losses, leaky_re_lu_70_layer_call_fn while saving (showing 5 of 34). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: data/models/demo2345_checkpoint/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: data/models/demo2345_checkpoint/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "193/193 [==============================] - 24s 118ms/step - loss: 0.0104 - val_loss: 0.0102\n",
      "Epoch 13/50\n",
      "193/193 [==============================] - ETA: 0s - loss: 0.0103\n",
      "Epoch 13: val_loss improved from 0.01015 to 0.01008, saving model to data/models/demo2345_checkpoint\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as leaky_re_lu_68_layer_call_fn, leaky_re_lu_68_layer_call_and_return_conditional_losses, leaky_re_lu_69_layer_call_fn, leaky_re_lu_69_layer_call_and_return_conditional_losses, leaky_re_lu_70_layer_call_fn while saving (showing 5 of 34). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: data/models/demo2345_checkpoint/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: data/models/demo2345_checkpoint/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "193/193 [==============================] - 24s 122ms/step - loss: 0.0103 - val_loss: 0.0101\n",
      "Epoch 14/50\n",
      "193/193 [==============================] - ETA: 0s - loss: 0.0102\n",
      "Epoch 14: val_loss improved from 0.01008 to 0.01001, saving model to data/models/demo2345_checkpoint\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as leaky_re_lu_68_layer_call_fn, leaky_re_lu_68_layer_call_and_return_conditional_losses, leaky_re_lu_69_layer_call_fn, leaky_re_lu_69_layer_call_and_return_conditional_losses, leaky_re_lu_70_layer_call_fn while saving (showing 5 of 34). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: data/models/demo2345_checkpoint/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: data/models/demo2345_checkpoint/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "193/193 [==============================] - 24s 120ms/step - loss: 0.0102 - val_loss: 0.0100\n",
      "Epoch 15/50\n",
      "193/193 [==============================] - ETA: 0s - loss: 0.0101\n",
      "Epoch 15: val_loss improved from 0.01001 to 0.00996, saving model to data/models/demo2345_checkpoint\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as leaky_re_lu_68_layer_call_fn, leaky_re_lu_68_layer_call_and_return_conditional_losses, leaky_re_lu_69_layer_call_fn, leaky_re_lu_69_layer_call_and_return_conditional_losses, leaky_re_lu_70_layer_call_fn while saving (showing 5 of 34). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: data/models/demo2345_checkpoint/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: data/models/demo2345_checkpoint/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "193/193 [==============================] - 24s 121ms/step - loss: 0.0101 - val_loss: 0.0100\n",
      "Epoch 16/50\n",
      "193/193 [==============================] - ETA: 0s - loss: 0.0101\n",
      "Epoch 16: val_loss improved from 0.00996 to 0.00991, saving model to data/models/demo2345_checkpoint\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as leaky_re_lu_68_layer_call_fn, leaky_re_lu_68_layer_call_and_return_conditional_losses, leaky_re_lu_69_layer_call_fn, leaky_re_lu_69_layer_call_and_return_conditional_losses, leaky_re_lu_70_layer_call_fn while saving (showing 5 of 34). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: data/models/demo2345_checkpoint/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: data/models/demo2345_checkpoint/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "193/193 [==============================] - 25s 123ms/step - loss: 0.0101 - val_loss: 0.0099\n",
      "Epoch 17/50\n",
      "193/193 [==============================] - ETA: 0s - loss: 0.0101"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [54]\u001b[0m, in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m num_classes \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m\n\u001b[1;32m      8\u001b[0m model \u001b[38;5;241m=\u001b[39m models\u001b[38;5;241m.\u001b[39mInceptionModel(batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m, test_split\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, num_classes\u001b[38;5;241m=\u001b[39mnum_classes, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m,\n\u001b[1;32m      9\u001b[0m                               lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m, seed\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, input_img_shape\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m32\u001b[39m, \u001b[38;5;241m32\u001b[39m, \u001b[38;5;241m32\u001b[39m,\u001b[38;5;241m5\u001b[39m))\n\u001b[0;32m---> 11\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdata/models/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mprefix\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_checkpoint\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdata/models/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mprefix\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_history.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m model\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata/models/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_epoch\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(model\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mevaluate(model\u001b[38;5;241m.\u001b[39mtesting_generator))\n",
      "File \u001b[0;32m~/code/final_project/photometric_redshift/final_project/models.py:173\u001b[0m, in \u001b[0;36mBaseModel.train\u001b[0;34m(self, checkpoint_path, history_path, workers)\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;124;03mWrapper for fitting the model with the generator we created\u001b[39;00m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;124;03mNote we can only use multiprocessing because we're passing in a generator\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[38;5;124;03m:param workers: int\u001b[39;00m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    167\u001b[0m my_callbacks \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    168\u001b[0m     tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mcallbacks\u001b[38;5;241m.\u001b[39mCSVLogger(history_path, separator\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m, append\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m),\n\u001b[1;32m    169\u001b[0m     tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mcallbacks\u001b[38;5;241m.\u001b[39mModelCheckpoint(filepath\u001b[38;5;241m=\u001b[39mcheckpoint_path, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, save_best_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m),\n\u001b[1;32m    170\u001b[0m     tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mcallbacks\u001b[38;5;241m.\u001b[39mEarlyStopping(monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m, min_delta\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0001\u001b[39m, patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    171\u001b[0m ]\n\u001b[0;32m--> 173\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_generator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtesting_generator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_multiprocessing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmy_callbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[43m    \u001b[49m\u001b[43mworkers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mworkers\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    181\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m history\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/photometric_redshift-qnFmoD7C/lib/python3.8/site-packages/keras/utils/traceback_utils.py:64\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 64\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/photometric_redshift-qnFmoD7C/lib/python3.8/site-packages/keras/engine/training.py:1420\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1406\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_eval_data_handler\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1407\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_eval_data_handler \u001b[38;5;241m=\u001b[39m data_adapter\u001b[38;5;241m.\u001b[39mget_data_handler(\n\u001b[1;32m   1408\u001b[0m       x\u001b[38;5;241m=\u001b[39mval_x,\n\u001b[1;32m   1409\u001b[0m       y\u001b[38;5;241m=\u001b[39mval_y,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1418\u001b[0m       model\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1419\u001b[0m       steps_per_execution\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_steps_per_execution)\n\u001b[0;32m-> 1420\u001b[0m val_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1421\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_x\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1422\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_y\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1423\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_sample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1424\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_batch_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1425\u001b[0m \u001b[43m    \u001b[49m\u001b[43msteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1426\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1427\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_queue_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_queue_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1428\u001b[0m \u001b[43m    \u001b[49m\u001b[43mworkers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mworkers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1429\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_multiprocessing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_multiprocessing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1430\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1431\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_use_cached_eval_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   1432\u001b[0m val_logs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m name: val \u001b[38;5;28;01mfor\u001b[39;00m name, val \u001b[38;5;129;01min\u001b[39;00m val_logs\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[1;32m   1433\u001b[0m epoch_logs\u001b[38;5;241m.\u001b[39mupdate(val_logs)\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/photometric_redshift-qnFmoD7C/lib/python3.8/site-packages/keras/utils/traceback_utils.py:64\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 64\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/photometric_redshift-qnFmoD7C/lib/python3.8/site-packages/keras/engine/training.py:1716\u001b[0m, in \u001b[0;36mModel.evaluate\u001b[0;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, return_dict, **kwargs)\u001b[0m\n\u001b[1;32m   1714\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m'\u001b[39m, step_num\u001b[38;5;241m=\u001b[39mstep, _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m   1715\u001b[0m   callbacks\u001b[38;5;241m.\u001b[39mon_test_batch_begin(step)\n\u001b[0;32m-> 1716\u001b[0m   tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtest_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1717\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[1;32m   1718\u001b[0m     context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/photometric_redshift-qnFmoD7C/lib/python3.8/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/photometric_redshift-qnFmoD7C/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    912\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    914\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 915\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    917\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    918\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/photometric_redshift-qnFmoD7C/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py:954\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    951\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    952\u001b[0m \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[1;32m    953\u001b[0m \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[0;32m--> 954\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_stateful_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    955\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_created_variables \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ALLOW_DYNAMIC_VARIABLE_CREATION:\n\u001b[1;32m    956\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating variables on a non-first call to a function\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    957\u001b[0m                    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m decorated with tf.function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/photometric_redshift-qnFmoD7C/lib/python3.8/site-packages/tensorflow/python/eager/function.py:2956\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2953\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m   2954\u001b[0m   (graph_function,\n\u001b[1;32m   2955\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m-> 2956\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2957\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/photometric_redshift-qnFmoD7C/lib/python3.8/site-packages/tensorflow/python/eager/function.py:1853\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1849\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1850\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1851\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1852\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1853\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1854\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   1855\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1856\u001b[0m     args,\n\u001b[1;32m   1857\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1858\u001b[0m     executing_eagerly)\n\u001b[1;32m   1859\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/photometric_redshift-qnFmoD7C/lib/python3.8/site-packages/tensorflow/python/eager/function.py:499\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    497\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    498\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 499\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    503\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    504\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    505\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    506\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m    507\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[1;32m    508\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    511\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[1;32m    512\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/photometric_redshift-qnFmoD7C/lib/python3.8/site-packages/tensorflow/python/eager/execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 54\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     57\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Model Training example, bypassing the Luigi Task for this:\n",
    "from final_project import models\n",
    "import datetime\n",
    "\n",
    "time = datetime.datetime.now()\n",
    "prefix = \"demo\"+str(time.hour)+str(time.minute)\n",
    "num_classes = 100\n",
    "model = models.InceptionModel(batch_size=32, test_split=0.2, num_classes=num_classes, epochs=50,\n",
    "                              lr=0.001, seed=1, input_img_shape=(32, 32, 32,5))\n",
    "\n",
    "history = model.train(f\"data/models/{prefix}_checkpoint\", f'data/models/{prefix}_history.csv')\n",
    "model.model.save(f\"data/models/{prefix}_epoch\")\n",
    "print(model.model.evaluate(model.testing_generator))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca2dad69",
   "metadata": {},
   "source": [
    "### Data Generator\n",
    "Thank you so much for your suggestion of that Stanford blog!! [3] This worked out so well. Even though my data ended up fitting in memory (~40MB, down from the 80GB raw), this still allowed for multiprocessing, as long as I kept the data generator thread-safe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9561774b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DataGenerator(tf.keras.utils.Sequence):\n",
    "    \"\"\"\n",
    "    Generator for Keras training to allow multiprocessing and training on batches with only the\n",
    "    batch itself being loaded into memory.\n",
    "\n",
    "    This implementation was heavily inspired by various examples for creating data generators\n",
    "    for keras, namely https://stanford.edu/~shervine/blog/keras-how-to-generate-data-on-the-fly.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        data_IDs: np.array,\n",
    "        labels: pd.Series,\n",
    "        n_classes: int,\n",
    "        batch_size: int = 64,\n",
    "        dim: tuple = (32, 32, 5),\n",
    "    ):\n",
    "        self.data_IDs = data_IDs\n",
    "        self.labels = labels\n",
    "        self.n_classes = n_classes\n",
    "        self.dim = dim\n",
    "        self.batch_size = batch_size\n",
    "        self.batch = self.get_batch()\n",
    "        self.indexes = self.get_indices()\n",
    "        self.shuffle = True\n",
    "\n",
    "    def get_batch(self):\n",
    "        \"\"\"\n",
    "        Generating upon initalization, so we can keep things thread-safe for multiprocessing.\n",
    "        This is a little messy, but I've tested the function pretty well\n",
    "        \"\"\"\n",
    "        batch = {}\n",
    "        num_batches = len(self.data_IDs) // self.batch_size\n",
    "        print(\"Leftover: \", len(self.data_IDs) % self.batch_size)\n",
    "        j = 1\n",
    "        for i in range(num_batches):\n",
    "            batch[i] = self.data_IDs[i * self.batch_size : j * self.batch_size]\n",
    "            j += 1\n",
    "\n",
    "        return batch\n",
    "\n",
    "    def get_indices(self):\n",
    "        \"\"\"\n",
    "        Returns indices for __getitem__. Want there to be the same number of indices as there are batches\n",
    "        to run.\n",
    "        \"\"\"\n",
    "        return np.arange(len(self.data_IDs) // self.batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Number of batches per epoch\"\"\"\n",
    "        return len(self.data_IDs) // self.batch_size\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"Generate one batch of data\"\"\"\n",
    "        X, y = self._data_generation(self.batch[index])\n",
    "\n",
    "        return X, y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        \"\"\"Updates indexes with shuffle after each epoch\"\"\"\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "    def _data_generation(self, batch):\n",
    "        \"\"\"\n",
    "        From file names for a batch, generate the data.\n",
    "        \"\"\"\n",
    "        X = np.empty((self.batch_size, *self.dim))\n",
    "        y = np.empty((self.batch_size))\n",
    "\n",
    "        # Generate data\n",
    "        for i, ID in enumerate(batch):\n",
    "            x = glob.glob(\"data/processed/\" + str(ID) + \"-*.npy\")[0]\n",
    "            # I saved the datacubes as (5, 32, 32) instead of (32, 32, 5) as they need to be for training\n",
    "            X[i] = np.transpose(np.load(os.path.join(abs_path, \"..\", x)), (1, 2, 0))\n",
    "\n",
    "            # ID is not entirely unique in the csv, so occasionally I get one objID with two spectroscopic redshifts\n",
    "            # I decided to handle this by averaging the estimates (it's always two)\n",
    "            if type(self.labels[ID]) == pd.Series:\n",
    "                y[i] = np.mean(self.labels[ID].values)\n",
    "            else:\n",
    "                y[i] = self.labels[ID]\n",
    "\n",
    "        return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3b16598",
   "metadata": {},
   "source": [
    "## Steps to Improve Model <a name=\"improving\"></a>\n",
    "Out of the box, the original architecture from Pasquet found at this [repo](https://github.com/umesh-timalsina/redshift/) did not work for me at all (you have to look at a few commits back to see their model before they transitioned the model to something called DeepForge).\n",
    "\n",
    "**The original paper's code used a sparse categorical cross entropy loss**, which neglects what our \"categories\" really mean - since an estimate that's in a nearby bin is better than an estimate that's in a far away bin. So the model is still outputing a rough PDF of redshift distribution, but the new loss function evaluates the MSE of the expected value of the redshift prediction as detailed here  [5]:\n",
    "<img src='docs/images/expectedvalue.png' width=\"700\" height=\"400\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e2c7a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This was the loss function I wrote for training instead of the sparse_categorical_crossentropy that the \n",
    "# paper repo was using.\n",
    "    def loss(self, true, pred):\n",
    "        \"\"\"\n",
    "        Custom loss function for keras\n",
    "        True should be a float, pred should be a (num_classes,) shaped array.\n",
    "        Calculates the MSE between the true value and the expected value of the output\n",
    "        \"\"\"\n",
    "        return tf.square(tf.subtract(true, tf.reduce_sum(tf.multiply(self.bin_values, pred))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "223cee3b",
   "metadata": {},
   "source": [
    "\n",
    "It was initially very discouraging results with the model, even with the modified loss function. I experienced results that often looked like this with completely flatlining training, even with the full architecture:\n",
    "<img src='docs/images/extraining.png' width=\"400\" height=\"400\">\n",
    "\n",
    "(Keep in mind that my redshift values range from 0 to 0.4.)\n",
    "\n",
    "The model would converge to often a terrible loss after only one epoch and I would never see the validation loss change. It would early stop after my patience=5 had been reached. I thought it might be a problem with the PRelu activations from the original paper dying off, especially as my loss is initially above 100 and drops dramatically in the first epoch. The validation loss would also always be just slightly below the training loss, which gave me pause. After rulling data leaks and other things out, I think that it's because the validation loss is being evaluated *after* the training for that epoch.\n",
    "\n",
    "I switched to LeakyRelu to avoid the vanishing gradients, but didn't see any change in behavior. I combed through my code to make sure that there weren't any errors, but couldn't find anything. \n",
    "\n",
    "#### Data Augmentation & Normalization:\n",
    "Experiments to tried:\n",
    "- smaller architecture\n",
    "    - One dense layer (instead of two)\n",
    "    - Three inception modules (instead of five)\n",
    "- smaller with normalized images\n",
    "    - computed statistics from entire dataset and passed into keras Normalization layer\n",
    "- smaller with normalized images and data augmentation\n",
    "    - used Keras RandomFlip layer with horizontal and vertical flipping\n",
    "- different hyperparameters: \n",
    "    - batch size: 32, 64\n",
    "    - learning rates: 0.0001, 0.0005, 0.001, 0.01, 1.0, and 10.0 \n",
    "    - epsilon: 1e-7 (default), 0.1, 1.0\n",
    "- different activation functions: \n",
    "    - PReLu\n",
    "    - LeakyReLu\n",
    "    - TanH (didn't work)\n",
    "    \n",
    "It consistently converged after only one epoch. Used early stopping for most of it (with a high patience of 5, saving best model checkpoint). Validation loss was usually smaller than training loss after the first epoch (loss would start at 100ish and drop to 1-6 range usually).\n",
    "\n",
    "I struggled with the convergence of the model after only one epoch with every variation - LR, normalization, batch size, leaky RELU, parametrized RELU, etc. Then I read that the InceptionV3 authors suggested using a very large epilson value for ADAM (1.0 or 0.1 instead of 1e-7) [1].\n",
    "\n",
    "Luckily, that was the breakthrough! While the normalization and data augmentation further improved things, the model went from converging in one epoch, often to very large losses (often above ten), to actually changing across epochs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a991ba8",
   "metadata": {},
   "source": [
    "## Results\n",
    "Overall, there were two models that stood out. Both had the same, smaller architecture, Adam lr of 0.0001, Adam epsilon of 1.0, and loss function, except the the RandomFlip keras data augmentation layer.\n",
    "\n",
    "**Below is a summary of a few models (and the best):**\n",
    "<img src='docs/images/results.png' width=\"600\" height=\"600\">\n",
    "\n",
    "While they both achieved very similar results, I think that the data augmented model will generalize better, as it had to learn to accomodate horizontal and vertical flips. It makes sense that the model should be agnostic to whether or not there was an extra mirror in the telescope or if the camera was mounted upside down.\n",
    "\n",
    "<img src='docs/images/predpdf.png' width=\"400\" height=\"400\"> \n",
    "\n",
    "Above is a visualized PDF of a prediction from the augmented model (it had predicted almost zero for the rest of the bins to the right, so the plot is \"zoomed in\").\n",
    "\n",
    "**Data Augmented Model Training History:**\n",
    "<img src='docs/images/most_acc_dataaug.png' width=\"400\" height=\"400\"> \n",
    "\n",
    "## Concerns\n",
    "The data is not very balanced. As you can see in the EDA section where we look at a histogram of the redshift values, there are very few in the dataset above z=0.3. Most of the data would be contained in eight bins (for the width I used) centered around z=0.1 in the final prediction layer. I would also like to evaluate the model with different metrics like Probability Integral Transform (PIT) and the Continuous Ranked Probability Score (CRPS) to examine how the model has really learned the distribution.  \n",
    "I'm also not sure if I incentivized the right behavior by softmaxing and then the MSE of the expected value loss that took this to a regression problem. The redshift problem is known to be a multimodal problem [6] and this loss function neglects that. In the future, I would like to find a loss that respects the nature of the problem more. I would also like to visualize the kernels being learned in the model to better understand what the model is really learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d451d03a",
   "metadata": {},
   "source": [
    "## Sources <a name=\"sources\"></a>\n",
    "[1] InceptionV3 paper with large epilson https://arxiv.org/abs/1512.00567  \n",
    "[2] Pasquet paper https://arxiv.org/abs/1806.06607  \n",
    "[3] Stanford data generator for keras https://stanford.edu/~shervine/blog/keras-how-to-generate-data-on-the-fly) \n",
    "\n",
    "[4] Pasquet repository with architecture https://github.com/umesh-timalsina/redshift/  \n",
    "[5] Expected Value of bins blurb https://www.aanda.org/articles/aa/full_html/2019/01/aa33617-18/aa33617-18.html  \n",
    "[6] Multimodal https://ui.adsabs.harvard.edu/abs/2016arXiv160606094K/abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b2aa2b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "virtenv",
   "language": "python",
   "name": "virtenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
